{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3935d2f4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-12T05:18:08.162604Z",
     "iopub.status.busy": "2025-02-12T05:18:08.162292Z",
     "iopub.status.idle": "2025-02-12T05:19:13.369254Z",
     "shell.execute_reply": "2025-02-12T05:19:13.367923Z"
    },
    "papermill": {
     "duration": 65.214307,
     "end_time": "2025-02-12T05:19:13.371952",
     "exception": false,
     "start_time": "2025-02-12T05:18:08.157645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m454.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mName: langchain-google-genai\r\n",
      "Version: 2.0.9\r\n",
      "Summary: An integration package connecting Google's genai package and LangChain\r\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MIT\r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: filetype, google-generativeai, langchain-core, pydantic\r\n",
      "Required-by: \r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\r\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\r\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-google-genai\n",
    "!pip install --upgrade -q langchain-google-genai\n",
    "!pip show langchain-google-genai\n",
    "!pip install -q google-generativeai\n",
    "!pip install -q python-dotenv\n",
    "! pip install -q langchain_community tiktoken langchainhub chromadb langchain langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099470c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T05:19:13.392978Z",
     "iopub.status.busy": "2025-02-12T05:19:13.392609Z",
     "iopub.status.idle": "2025-02-12T05:19:16.574119Z",
     "shell.execute_reply": "2025-02-12T05:19:16.573351Z"
    },
    "papermill": {
     "duration": 3.194323,
     "end_time": "2025-02-12T05:19:16.575898",
     "exception": false,
     "start_time": "2025-02-12T05:19:13.381575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tracers import LangChainTracer\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2035f41a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T05:19:16.588652Z",
     "iopub.status.busy": "2025-02-12T05:19:16.588102Z",
     "iopub.status.idle": "2025-02-12T05:19:16.904931Z",
     "shell.execute_reply": "2025-02-12T05:19:16.903954Z"
    },
    "papermill": {
     "duration": 0.324909,
     "end_time": "2025-02-12T05:19:16.906733",
     "exception": false,
     "start_time": "2025-02-12T05:19:16.581824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = UserSecretsClient().get_secret('GOOGLE_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = UserSecretsClient().get_secret('LANGSMITH_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-prickly-example-65\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3cb3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T05:19:16.919046Z",
     "iopub.status.busy": "2025-02-12T05:19:16.918731Z",
     "iopub.status.idle": "2025-02-12T05:19:20.955911Z",
     "shell.execute_reply": "2025-02-12T05:19:20.954870Z"
    },
    "papermill": {
     "duration": 4.045212,
     "end_time": "2025-02-12T05:19:20.957804",
     "exception": false,
     "start_time": "2025-02-12T05:19:16.912592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01519c8",
   "metadata": {
    "papermill": {
     "duration": 0.005314,
     "end_time": "2025-02-12T05:19:20.969111",
     "exception": false,
     "start_time": "2025-02-12T05:19:20.963797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multi Query: Different Perspectives\n",
    "This approach enhances document retrieval by generating multiple reworded versions of a user query, helping overcome the limitations of distance-based similarity searches in vector databases. A predefined prompt template instructs the AI model (Gemini Pro) to create five alternative queries. The output is processed by a structured pipeline, ensuring well-formatted variations that improve search accuracy and relevance. With a deterministic temperature setting, the responses remain consistent, making the retrieval system more effective. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251c6af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T05:19:20.981437Z",
     "iopub.status.busy": "2025-02-12T05:19:20.980819Z",
     "iopub.status.idle": "2025-02-12T05:19:20.990175Z",
     "shell.execute_reply": "2025-02-12T05:19:20.989342Z"
    },
    "papermill": {
     "duration": 0.017284,
     "end_time": "2025-02-12T05:19:20.991893",
     "exception": false,
     "start_time": "2025-02-12T05:19:20.974609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319a177",
   "metadata": {
    "papermill": {
     "duration": 0.005312,
     "end_time": "2025-02-12T05:19:21.003479",
     "exception": false,
     "start_time": "2025-02-12T05:19:20.998167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Chain to generate 5 different versions of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c567c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T05:19:21.015591Z",
     "iopub.status.busy": "2025-02-12T05:19:21.015204Z",
     "iopub.status.idle": "2025-02-12T05:19:23.346013Z",
     "shell.execute_reply": "2025-02-12T05:19:23.344673Z"
    },
    "papermill": {
     "duration": 2.341302,
     "end_time": "2025-02-12T05:19:23.350191",
     "exception": false,
     "start_time": "2025-02-12T05:19:21.008889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-bcab5aaf3a68>:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1a161",
   "metadata": {
    "papermill": {
     "duration": 0.010032,
     "end_time": "2025-02-12T05:19:23.372610",
     "exception": false,
     "start_time": "2025-02-12T05:19:23.362578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Combining 5 subqueries to get a single consolidated subquery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e0fb99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T05:19:23.388700Z",
     "iopub.status.busy": "2025-02-12T05:19:23.388148Z",
     "iopub.status.idle": "2025-02-12T05:19:27.050250Z",
     "shell.execute_reply": "2025-02-12T05:19:27.049292Z"
    },
    "papermill": {
     "duration": 3.672165,
     "end_time": "2025-02-12T05:19:27.051883",
     "exception": false,
     "start_time": "2025-02-12T05:19:23.379718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4b684",
   "metadata": {
    "papermill": {
     "duration": 0.005768,
     "end_time": "2025-02-12T05:19:27.063595",
     "exception": false,
     "start_time": "2025-02-12T05:19:27.057827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 82.910717,
   "end_time": "2025-02-12T05:19:28.291793",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-12T05:18:05.381076",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
