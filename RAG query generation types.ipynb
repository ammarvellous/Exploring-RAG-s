{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain-google-genai\n!pip install --upgrade -q langchain-google-genai\n!pip show langchain-google-genai\n!pip install -q google-generativeai\n!pip install -q python-dotenv\n! pip install -q langchain_community tiktoken langchainhub chromadb langchain langsmith","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.tracers import LangChainTracer\nfrom langchain.prompts import ChatPromptTemplate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ['GOOGLE_API_KEY'] = UserSecretsClient().get_secret('GOOGLE_API_KEY')\nos.environ['LANGCHAIN_API_KEY'] = UserSecretsClient().get_secret('LANGSMITH_API_KEY')\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"pr-prickly-example-65\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:40:00.128786Z","iopub.execute_input":"2025-02-12T05:40:00.129188Z","iopub.status.idle":"2025-02-12T05:40:00.527929Z","shell.execute_reply.started":"2025-02-12T05:40:00.129150Z","shell.execute_reply":"2025-02-12T05:40:00.526924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load blog\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\nblog_docs = loader.load()\n\n# Split\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=300, \n    chunk_overlap=50)\n\n# Make splits\nsplits = text_splitter.split_documents(blog_docs)\n\n# Index\nvectorstore = Chroma.from_documents(documents=splits, \n                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:10:25.008439Z","iopub.execute_input":"2025-02-12T05:10:25.009080Z","iopub.status.idle":"2025-02-12T05:10:26.092378Z","shell.execute_reply.started":"2025-02-12T05:10:25.009028Z","shell.execute_reply":"2025-02-12T05:10:26.091288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi Query: Different Perspectives\nThis approach enhances document retrieval by generating multiple reworded versions of a user query, helping overcome the limitations of distance-based similarity searches in vector databases. A predefined prompt template instructs the AI model (Gemini Pro) to create five alternative queries. The output is processed by a structured pipeline, ensuring well-formatted variations that improve search accuracy and relevance. With a deterministic temperature setting, the responses remain consistent, making the retrieval system more effective. üöÄ","metadata":{}},{"cell_type":"code","source":"\ntemplate = \"\"\"You are an AI language model assistant. Your task is to generate five \ndifferent versions of the given user question to retrieve relevant documents from a vector \ndatabase. By generating multiple perspectives on the user question, your goal is to help\nthe user overcome some of the limitations of the distance-based similarity search. \nProvide these alternative questions separated by newlines. Original question: {question}\"\"\"\nprompt_perspectives = ChatPromptTemplate.from_template(template)\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n\ngenerate_queries = (\n    prompt_perspectives \n    | llm\n    | StrOutputParser() \n    | (lambda x: x.split(\"\\n\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:10:29.614543Z","iopub.execute_input":"2025-02-12T05:10:29.614895Z","iopub.status.idle":"2025-02-12T05:10:29.625304Z","shell.execute_reply.started":"2025-02-12T05:10:29.614872Z","shell.execute_reply":"2025-02-12T05:10:29.623885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chain to generate 5 different versions of the query","metadata":{}},{"cell_type":"code","source":"from langchain.load import dumps, loads\n\ndef get_unique_union(documents: list[list]):\n    \"\"\" Unique union of retrieved docs \"\"\"\n    # Flatten list of lists, and convert each Document to string\n    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n    # Get unique documents\n    unique_docs = list(set(flattened_docs))\n    # Return\n    return [loads(doc) for doc in unique_docs]\n\n# Retrieve\nquestion = \"What is task decomposition for LLM agents?\"\nretrieval_chain = generate_queries | retriever.map() | get_unique_union\ndocs = retrieval_chain.invoke({\"question\":question})\nlen(docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:10:40.471956Z","iopub.execute_input":"2025-02-12T05:10:40.472274Z","iopub.status.idle":"2025-02-12T05:10:42.242361Z","shell.execute_reply.started":"2025-02-12T05:10:40.472250Z","shell.execute_reply":"2025-02-12T05:10:42.241300Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Combining 5 subqueries to get a single consolidated subquery ","metadata":{}},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.runnables import RunnablePassthrough\n\n# RAG\ntemplate = \"\"\"Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n\nfinal_rag_chain = (\n    {\"context\": retrieval_chain, \n     \"question\": itemgetter(\"question\")} \n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nfinal_rag_chain.invoke({\"question\":question})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:52:45.212851Z","iopub.execute_input":"2025-02-12T04:52:45.213206Z","iopub.status.idle":"2025-02-12T04:52:51.264657Z","shell.execute_reply.started":"2025-02-12T04:52:45.213182Z","shell.execute_reply":"2025-02-12T04:52:51.263513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG fusion","metadata":{}},{"cell_type":"markdown","source":"The function reciprocal_rank_fusion takes:\n\nresults: A list of ranked document lists (i.e., multiple lists of search results).\nk: A smoothing parameter (default is 60) used in the RRF formula.\n\nWe create an empty dictionary fused_scores to store documents and their combined scores.\n\nEach docs represents one ranked list of documents.\nWe will process each document within these lists.\n\nWe loop through each document in the ranked list and get its position (rank).\n\nConverts the document into a string format for consistent indexing in the dictionary (since documents can be complex objects).\n\nIf the document is not already in the dictionary, initialize its score to 0.\n\nFormula Used:\nRRF¬†Score=1/(rank+ùëò)\n\n \nExplanation:\nDocuments appearing at higher ranks (lower rank values) get higher scores.(top ranks i.e 1, 2, 3, ...)\nThe k parameter ensures no division by zero and smooths the ranking.\nThis way, even if a document ranks low in one list, it still contributes if it appears across multiple lists.\n\nConverts the document strings back into objects (loads(doc)) and sorts them in descending order based on their final scores.\nThe highest-scoring documents are ranked at the top.\n\nThe function returns a list of tuples, where each tuple contains:\nThe document\nIts final RRF score","metadata":{}},{"cell_type":"markdown","source":"Significance in Context\nThe RRF function is useful in Retrieval-Augmented Generation (RAG) workflows.\n\nIt helps merge multiple search results to improve relevance.\nIt does not require exact relevance scores‚Äîonly the ranking order matters.\nUsed commonly in search engines, question-answering models, and AI retrieval systems.","metadata":{}},{"cell_type":"code","source":"from langchain.load import dumps, loads\n\ndef reciprocal_rank_fusion(results: list[list], k=60):\n    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n        and an optional parameter k used in the RRF formula \"\"\"\n    \n    # Initialize a dictionary to hold fused scores for each unique document\n    fused_scores = {}\n\n    # Iterate through each list of ranked documents\n    for docs in results:\n        # Iterate through each document in the list, with its rank (position in the list)\n        for rank, doc in enumerate(docs):\n            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n            doc_str = dumps(doc)\n            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n            if doc_str not in fused_scores:\n                fused_scores[doc_str] = 0\n            # Retrieve the current score of the document, if any\n            previous_score = fused_scores[doc_str]\n            # Update the score of the document using the RRF formula: 1 / (rank + k)\n            fused_scores[doc_str] += 1 / (rank + k)\n\n    # Sort the documents based on their fused scores in descending order to get the final reranked results\n    reranked_results = [\n        (loads(doc), score)\n        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n    ]\n\n    # Return the reranked results as a list of tuples, each containing the document and its fused score\n    return reranked_results\n\nretrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\ndocs = retrieval_chain_rag_fusion.invoke({\"question\": question})\nlen(docs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:29:40.780122Z","iopub.execute_input":"2025-02-12T05:29:40.780508Z","iopub.status.idle":"2025-02-12T05:29:42.519325Z","shell.execute_reply.started":"2025-02-12T05:29:40.780482Z","shell.execute_reply":"2025-02-12T05:29:42.518124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"note in above scenario:\ndo not get confused, we are just, getting the past score if it exists, but simply not using it, \nas the rank as being summed along the docs,\nso think of it like a cumulative sum happening across the doc, to get the overall rank from all the doc per question","metadata":{}},{"cell_type":"code","source":"from langchain_core.runnables import RunnablePassthrough\nfrom operator import itemgetter\n# RAG\ntemplate = \"\"\"Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nfinal_rag_chain = (\n    {\"context\": retrieval_chain_rag_fusion, \n     \"question\": itemgetter(\"question\")} \n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nfinal_rag_chain.invoke({\"question\":question})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:31:17.134814Z","iopub.execute_input":"2025-02-12T05:31:17.135268Z","iopub.status.idle":"2025-02-12T05:31:19.879610Z","shell.execute_reply.started":"2025-02-12T05:31:17.135236Z","shell.execute_reply":"2025-02-12T05:31:19.878336Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Decomposition","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\n\n# Decomposition\ntemplate = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\nGenerate multiple search queries related to: {question} \\n\nOutput (3 queries):\"\"\"\nprompt_decomposition = ChatPromptTemplate.from_template(template)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:37:44.798752Z","iopub.execute_input":"2025-02-12T05:37:44.799131Z","iopub.status.idle":"2025-02-12T05:37:44.804936Z","shell.execute_reply.started":"2025-02-12T05:37:44.799101Z","shell.execute_reply":"2025-02-12T05:37:44.803519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LLM\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:43:44.866965Z","iopub.execute_input":"2025-02-12T05:43:44.867388Z","iopub.status.idle":"2025-02-12T05:43:44.874953Z","shell.execute_reply.started":"2025-02-12T05:43:44.867353Z","shell.execute_reply":"2025-02-12T05:43:44.873605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chain\ngenerate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n\n# Run\nquestion = \"What are the main components of an LLM-powered autonomous agent system?\"\nquestions = generate_queries_decomposition.invoke({\"question\":question})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:40:36.699138Z","iopub.execute_input":"2025-02-12T05:40:36.699567Z","iopub.status.idle":"2025-02-12T05:40:37.743708Z","shell.execute_reply.started":"2025-02-12T05:40:36.699535Z","shell.execute_reply":"2025-02-12T05:40:37.742600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:40:55.443602Z","iopub.execute_input":"2025-02-12T05:40:55.443943Z","iopub.status.idle":"2025-02-12T05:40:55.450625Z","shell.execute_reply.started":"2025-02-12T05:40:55.443916Z","shell.execute_reply":"2025-02-12T05:40:55.449111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Prompt\ntemplate = \"\"\"Here is the question you need to answer:\n\n\\n --- \\n {question} \\n --- \\n\n\nHere is any available background question + answer pairs:\n\n\\n --- \\n {q_a_pairs} \\n --- \\n\n\nHere is additional context relevant to the question: \n\n\\n --- \\n {context} \\n --- \\n\n\nUse the above context and any background question + answer pairs to answer the question: \\n {question}\n\"\"\"\n\ndecomposition_prompt = ChatPromptTemplate.from_template(template)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:41:38.138556Z","iopub.execute_input":"2025-02-12T05:41:38.138902Z","iopub.status.idle":"2025-02-12T05:41:38.144539Z","shell.execute_reply.started":"2025-02-12T05:41:38.138877Z","shell.execute_reply":"2025-02-12T05:41:38.143108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.output_parsers import StrOutputParser\n\ndef format_qa_pair(question, answer):\n    \"\"\"Format Q and A pair\"\"\"\n    \n    formatted_string = \"\"\n    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n    return formatted_string.strip()\n\nq_a_pairs = \"\"\nfor q in questions:\n    \n    rag_chain = (\n    {\"context\": itemgetter(\"question\") | retriever, \n     \"question\": itemgetter(\"question\"),\n     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n    | decomposition_prompt\n    | llm\n    | StrOutputParser())\n\n    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n    q_a_pair = format_qa_pair(q,answer)\n    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:43:51.004066Z","iopub.execute_input":"2025-02-12T05:43:51.004425Z","iopub.status.idle":"2025-02-12T05:44:07.614632Z","shell.execute_reply.started":"2025-02-12T05:43:51.004382Z","shell.execute_reply":"2025-02-12T05:44:07.613131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:44:07.616364Z","iopub.execute_input":"2025-02-12T05:44:07.616791Z","iopub.status.idle":"2025-02-12T05:44:07.622812Z","shell.execute_reply.started":"2025-02-12T05:44:07.616760Z","shell.execute_reply":"2025-02-12T05:44:07.621888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Answer individually","metadata":{}},{"cell_type":"code","source":"# Answer each sub-question individually \n\nfrom langchain import hub\n\n# RAG prompt\nprompt_rag = hub.pull(\"rlm/rag-prompt\")\n\ndef retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n    \"\"\"RAG on each sub-question\"\"\"\n    \n    # Use our decomposition / \n    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n    \n    # Initialize a list to hold RAG chain results\n    rag_results = []\n    \n    for sub_question in sub_questions:\n        \n        # Retrieve documents for each sub-question\n        retrieved_docs = retriever.get_relevant_documents(sub_question)\n        \n        # Use retrieved documents and sub-question in RAG chain\n        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n                                                                \"question\": sub_question})\n        rag_results.append(answer)\n    \n    return rag_results,sub_questions\n\n# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\nanswers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T05:45:19.017879Z","iopub.execute_input":"2025-02-12T05:45:19.018258Z","iopub.status.idle":"2025-02-12T05:45:21.614309Z","shell.execute_reply.started":"2025-02-12T05:45:19.018230Z","shell.execute_reply":"2025-02-12T05:45:21.612270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}