{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b64003",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-11T05:49:58.901213Z",
     "iopub.status.busy": "2025-02-11T05:49:58.900772Z",
     "iopub.status.idle": "2025-02-11T05:49:59.927640Z",
     "shell.execute_reply": "2025-02-11T05:49:59.926500Z"
    },
    "papermill": {
     "duration": 1.034428,
     "end_time": "2025-02-11T05:49:59.929636",
     "exception": false,
     "start_time": "2025-02-11T05:49:58.895208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca16637d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:49:59.939325Z",
     "iopub.status.busy": "2025-02-11T05:49:59.938714Z",
     "iopub.status.idle": "2025-02-11T05:51:17.260409Z",
     "shell.execute_reply": "2025-02-11T05:51:17.258893Z"
    },
    "papermill": {
     "duration": 77.328768,
     "end_time": "2025-02-11T05:51:17.262610",
     "exception": false,
     "start_time": "2025-02-11T05:49:59.933842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mName: langchain-google-genai\r\n",
      "Version: 2.0.9\r\n",
      "Summary: An integration package connecting Google's genai package and LangChain\r\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MIT\r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: filetype, google-generativeai, langchain-core, pydantic\r\n",
      "Required-by: \r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\r\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\r\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-google-genai\n",
    "!pip install --upgrade -q langchain-google-genai\n",
    "!pip show langchain-google-genai\n",
    "!pip install -q google-generativeai\n",
    "!pip install -q python-dotenv\n",
    "! pip install -q langchain_community tiktoken langchainhub chromadb langchain langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21383619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:17.278838Z",
     "iopub.status.busy": "2025-02-11T05:51:17.278478Z",
     "iopub.status.idle": "2025-02-11T05:51:17.635662Z",
     "shell.execute_reply": "2025-02-11T05:51:17.634526Z"
    },
    "papermill": {
     "duration": 0.367724,
     "end_time": "2025-02-11T05:51:17.637730",
     "exception": false,
     "start_time": "2025-02-11T05:51:17.270006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "os.environ['GOOGLE_API_KEY'] = UserSecretsClient().get_secret('GOOGLE_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = UserSecretsClient().get_secret('LANGSMITH_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-prickly-example-65\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1afb2",
   "metadata": {
    "papermill": {
     "duration": 0.006813,
     "end_time": "2025-02-11T05:51:17.651935",
     "exception": false,
     "start_time": "2025-02-11T05:51:17.645122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "bs4 (BeautifulSoup): Helps parse and extract specific parts of a webpage.\n",
    "langchain.hub: Provides access to reusable components in LangChain.\n",
    "RecursiveCharacterTextSplitter: Splits large text into smaller chunks so it's easier to work with.\n",
    "WebBaseLoader: Loads data from a webpage.\n",
    "Chroma: A vector database for storing and retrieving embeddings (numerical representations of text).\n",
    "StrOutputParser: Converts outputs into plain text.\n",
    "RunnablePassthrough: Passes data through a pipeline without modification.\n",
    "ChatOpenAI & OpenAIEmbeddings: Handles OpenAI's chatbot and text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d45bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:17.669154Z",
     "iopub.status.busy": "2025-02-11T05:51:17.668542Z",
     "iopub.status.idle": "2025-02-11T05:51:21.290444Z",
     "shell.execute_reply": "2025-02-11T05:51:21.288738Z"
    },
    "papermill": {
     "duration": 3.633214,
     "end_time": "2025-02-11T05:51:21.293132",
     "exception": false,
     "start_time": "2025-02-11T05:51:17.659918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tracers import LangChainTracer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6404fc9",
   "metadata": {
    "papermill": {
     "duration": 0.006939,
     "end_time": "2025-02-11T05:51:21.308651",
     "exception": false,
     "start_time": "2025-02-11T05:51:21.301712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading Documents from a Website:\n",
    "\n",
    "WebBaseLoader fetches content from the given blog post URL:\n",
    "👉 Lilian Weng’s blog on AI Agents\n",
    "bs_kwargs tells BeautifulSoup to extract only certain parts of the webpage:\n",
    "post-content: The main body of the blog.\n",
    "post-title: The title of the blog.\n",
    "post-header: Any headers at the top of the blog.\n",
    "docs = loader.load()\n",
    "\n",
    "This actually downloads and extracts the relevant text from the blog post.\n",
    "The result (docs) is a structured list of text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0be82c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:21.326985Z",
     "iopub.status.busy": "2025-02-11T05:51:21.326389Z",
     "iopub.status.idle": "2025-02-11T05:51:21.938480Z",
     "shell.execute_reply": "2025-02-11T05:51:21.937388Z"
    },
    "papermill": {
     "duration": 0.624802,
     "end_time": "2025-02-11T05:51:21.940598",
     "exception": false,
     "start_time": "2025-02-11T05:51:21.315796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4a1b6",
   "metadata": {
    "papermill": {
     "duration": 0.00821,
     "end_time": "2025-02-11T05:51:21.956736",
     "exception": false,
     "start_time": "2025-02-11T05:51:21.948526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Step 1: Splitting the Text for Better Processing\n",
    "\n",
    "🔹 Why do this?\n",
    "Long documents can be hard for language models to handle all at once, so we break them into smaller chunks.\n",
    "\n",
    "chunk_size=1000: Each chunk contains 1000 characters.\n",
    "chunk_overlap=200: Each chunk overlaps the next one by 200 characters to keep context intact.\n",
    "split_documents(docs): Actually splits the loaded blog text into these chunks.\n",
    "👉 Now we have smaller, manageable pieces of text that can be efficiently searched and retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ac1700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:21.972594Z",
     "iopub.status.busy": "2025-02-11T05:51:21.971933Z",
     "iopub.status.idle": "2025-02-11T05:51:21.980120Z",
     "shell.execute_reply": "2025-02-11T05:51:21.978895Z"
    },
    "papermill": {
     "duration": 0.018051,
     "end_time": "2025-02-11T05:51:21.981931",
     "exception": false,
     "start_time": "2025-02-11T05:51:21.963880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e40e7c",
   "metadata": {
    "papermill": {
     "duration": 0.006881,
     "end_time": "2025-02-11T05:51:21.996482",
     "exception": false,
     "start_time": "2025-02-11T05:51:21.989601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating a Vector Database\n",
    "\n",
    "🔹 Why do this?\n",
    "Raw text is hard to search directly, so we convert it into numerical representations (embeddings).\n",
    "\n",
    "GoogleGenerativeAIEmbeddings(): Uses gemini model to generate vector representations of each text chunk.\n",
    "Chroma.from_documents(...): Stores these embeddings in a Chroma vector database.\n",
    "👉 Now we have a searchable database, where each chunk of text is represented as a mathematical vector.\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "🔹 What does this do?\n",
    "\n",
    "Converts the vector database into a retriever, meaning we can now search for relevant text chunks by providing a query.\n",
    "👉 Now, if we ask a question, the retriever will find the most relevant pieces of text from our stored embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ce0107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:22.012327Z",
     "iopub.status.busy": "2025-02-11T05:51:22.011913Z",
     "iopub.status.idle": "2025-02-11T05:51:23.985400Z",
     "shell.execute_reply": "2025-02-11T05:51:23.984038Z"
    },
    "papermill": {
     "duration": 1.983809,
     "end_time": "2025-02-11T05:51:23.987751",
     "exception": false,
     "start_time": "2025-02-11T05:51:22.003942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37152fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:24.004630Z",
     "iopub.status.busy": "2025-02-11T05:51:24.004228Z",
     "iopub.status.idle": "2025-02-11T05:51:24.011735Z",
     "shell.execute_reply": "2025-02-11T05:51:24.010491Z"
    },
    "papermill": {
     "duration": 0.018343,
     "end_time": "2025-02-11T05:51:24.013797",
     "exception": false,
     "start_time": "2025-02-11T05:51:23.995454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"You are an AI assistant. Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c63807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:24.029289Z",
     "iopub.status.busy": "2025-02-11T05:51:24.028854Z",
     "iopub.status.idle": "2025-02-11T05:51:24.043958Z",
     "shell.execute_reply": "2025-02-11T05:51:24.042830Z"
    },
    "papermill": {
     "duration": 0.025005,
     "end_time": "2025-02-11T05:51:24.045885",
     "exception": false,
     "start_time": "2025-02-11T05:51:24.020880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tracer\n",
    "tracer = LangChainTracer()\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5951f28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:24.062484Z",
     "iopub.status.busy": "2025-02-11T05:51:24.062089Z",
     "iopub.status.idle": "2025-02-11T05:51:25.650448Z",
     "shell.execute_reply": "2025-02-11T05:51:25.649161Z"
    },
    "papermill": {
     "duration": 1.59888,
     "end_time": "2025-02-11T05:51:25.652548",
     "exception": false,
     "start_time": "2025-02-11T05:51:24.053668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done by the LLM with simple prompting, by using task-specific instructions, or with human inputs.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable  # This enables LangSmith tracking\n",
    "def run_rag():\n",
    "    return rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "response = run_rag()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5c8287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:51:25.669566Z",
     "iopub.status.busy": "2025-02-11T05:51:25.669088Z",
     "iopub.status.idle": "2025-02-11T05:51:25.677549Z",
     "shell.execute_reply": "2025-02-11T05:51:25.676256Z"
    },
    "papermill": {
     "duration": 0.019625,
     "end_time": "2025-02-11T05:51:25.680164",
     "exception": false,
     "start_time": "2025-02-11T05:51:25.660539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracing is enabled: true\n",
      "LangSmith project: pr-prickly-example-65\n",
      "LangSmith API Key set: True\n"
     ]
    }
   ],
   "source": [
    "import langsmith\n",
    "\n",
    "print(\"LangSmith tracing is enabled:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
    "print(\"LangSmith project:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "print(\"LangSmith API Key set:\", bool(os.getenv(\"LANGCHAIN_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5484b4",
   "metadata": {
    "papermill": {
     "duration": 0.007427,
     "end_time": "2025-02-11T05:51:25.695167",
     "exception": false,
     "start_time": "2025-02-11T05:51:25.687740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 91.57466,
   "end_time": "2025-02-11T05:51:27.430504",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-11T05:49:55.855844",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
